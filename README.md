# regime-eco-data

I would like to start off this repository by noting that this is the first model I have created. My goal here is to bridge together many complex concepts purely through what I have read and studied over the years. I did use AI to assist in the coding process. While I understand the basics of Python, AI helped me implement the structure I had in mind, and I believe the code captures what I intended it to do. Of course, there may still be mistakes in both the coding and conceptual framework, and I am aware of the gaps, for example the manual weighting approach used in grading indicators.
The purpose of this model is to capture changes in leading and coincident economic data sourced from FRED. Although this data is very clean and widely available, meaning the “alpha” is already priced in, it remains extremely important. Businesses, policymakers, and investors rely on this information to guide their decisions, which in turn influences the broader economy and asset prices. My framework takes these economic indicators and compares them against asset performance to see how markets react under different macroeconomic conditions.
This project is designed to create a systematic way of connecting macroeconomic data with financial markets. It collects and processes leading and coincident indicators from FRED, normalizes and aggregates them into composite scores to represent the current and forward-looking state of the economy, and uses statistical and machine learning techniques to classify regimes such as Expansion, Early Recovery, Late Cycle, and Crisis. It then tests how major asset classes, equities, treasuries, cash, and gold, behave in these different regimes. In short, the project is meant to serve as an experimental framework for exploring how the economy and markets interact, and to see what assets perform best under different macroeconomic themes. While it is not a finished or perfect product, it reflects my attempt to put theory into practice and build a foundation for more advanced models in the future.
Now, to be transparent, there are some clear problems with this model. The first is that I rely entirely on clean and public data. Everyone has access to FRED, and most large players have alternative datasets that allow them to estimate or even front-run the official releases of major indicators. As a result, asset prices are often adjusted well before the data even comes out. What actually moves markets is not the data itself, but the surprise, the difference between expectation and reality. In practice, this means that while everyone can brace for the expected, the real volatility comes from when things turn out far better or worse than anticipated. This makes my model more descriptive than predictive, and highlights the psychological dimension of markets.
The second problem is the manual input of weights and indicator selection. This makes the framework highly fragile, as a small change in inputs could completely flip the output, meaning I could just as easily convince myself of patterns that are not really there. It serves as a reminder that with enough tweaking, you can make almost anything look meaningful, something my mom likes to point out when she tells me I read way too deeply into things. In seriousness, I recognize this as an area where machine learning could help automate the weighting process and reduce bias. At this stage, I am still learning, but I plan to explore that path further.


1) Economic Indicators
Coincident Indicators - CoincedentIndicators (1).ipynb
I chose coincident indicators because they measure the economy as it currently stands, providing a real-time pulse of activity. They are less about forecasting and more about capturing the “now,” which is important for aligning financial markets with actual economic momentum. The challenge with this type of data is that many market participants already have ways of estimating these measures in real time, rather than waiting for the official releases. In that sense, I am at a disadvantage. However, these indicators remain widely used and closely followed, and that alone makes them important. Since, markets move based on what other participants believe is important. The main issue is that sophisticated players often develop methods to anticipate these releases ahead of the broader public such as alternative data. 
GDP Growth
This is the broadest measure of economic output. From a theoretical standpoint, GDP is the ultimate coincident benchmark because it represents the total value of goods and services produced. Practically, investors use it as a baseline reference for whether the economy is expanding or contracting.
Nonfarm Payrolls
Employment is a critical component of demand and overall confidence. It ties directly into income and spending, making it one of the most closely watched coincident data points by policymakers and markets.
Retail Sales
Retail sales serve as a proxy for consumer demand in the goods economy. Theoretically, consumption accounts for the majority of GDP, so this metric reflects a large portion of economic activity. Practically, it gives high-frequency insight into household behavior.
PCE (Personal Consumption Expenditures)
This measure is broader than retail sales because it captures both goods and services. PCE also matters because it is the Federal Reserve’s preferred inflation and consumption gauge, tying it directly to monetary policy.
Industrial Production
Industrial production captures the supply side of the economy, particularly cyclical industries such as manufacturing, mining, and utilities. It is sensitive to demand shocks and often reflects shifts in business cycles.
Disposable Personal Income
This indicator represents household purchasing power after taxes. Theoretically, it anchors sustainable consumption growth. Practically, it shows whether households have the ability to keep fueling spending.
Corporate Profits
Profits are the lifeblood of business investment and equity valuations. They provide a bridge between the real economy and financial markets, since higher profits typically support higher stock prices and reinvestment.
Together, these indicators form a broad real-time map of the economy’s health across households, businesses, and production. While you could argue that I am missing some additional measures, I believe these capture what is most important for understanding the current state of the economy and provide the clearest picture of its health. Since the Federal Reserve also relies on many of these same indicators, tracking them helps me better anticipate how policymakers may view the economy.
Leading Indicators- LeadingIndicators (1).ipynb
I chose leading indicators because they tend to turn before the broader economy, offering predictive insight into upcoming shifts in the cycle. They are widely used by economists, portfolio managers, and policymakers to anticipate recessions, recoveries, or overheating conditions. While they serve as warnings, their true importance lies in how businesses and other market participants use them to adjust positions across different assets and strategies. Since many players rely on these indicators to form expectations about the future, they become powerful not just for what they measure, but for the collective reaction they trigger. For example, when the yield curve inverts, the exact moment of inversion matters less than the potential chain of feedback loops and market behaviors that may follow.
My perspective here is influenced by Soros’s framework of reflexivity and feedback loops, where people’s anticipation of certain events can actually bring those events closer to reality. If leading data suggests rough waters ahead, market participants often begin preparing for that scenario. As more players adjust simultaneously, their actions can create a feedback loop that makes the downturn more likely to materialize. Of course, leading indicators can generate false positives, sometimes frequently, but even then I want to understand how those signals influence positioning and behavior across the market.
Yield Curve Spreads (10Y–2Y, 10Y–3M)
These are theoretically tied to expectations of future growth and interest rates. When inverted, they signal tighter financial conditions and recession risks. Practically, they are among the most reliable predictors of U.S. recessions. 
M2 Money Supply Growth
This indicator reflects liquidity and credit availability in the system. From a theoretical perspective, abundant liquidity can stimulate growth and inflation; practically, slowing M2 has often preceded weaker economic momentum.
Durable Goods Orders
Durable goods act as a proxy for business investment plans. Businesses commit to machinery and equipment only when they have confidence in future demand. A decline signals caution about upcoming conditions.
Building Permits
Housing is highly cyclical and interest-rate sensitive, with ripple effects spreading into consumption, labor, and credit markets. Permits are one of the earliest signs of housing cycles, making them a strong forward-looking measure.
Consumer Confidence
Consumer confidence captures household expectations about the economy and their financial situation. Theoretically important because expectations often shape future spending. Practically, confidence collapses often foreshadow downturns in demand.
Together, these indicators map the economy’s likely trajectory and help anticipate turning points before they become visible in coincident data. I am not claiming to predict the future or to use this information as a crystal ball, but I do believe it is important to pay close attention to these signals. The real value comes from how they shape expectations and the potential second-order effects that follow. Businesses and policymakers rely heavily on this data to guide decisions, which in turn influences investment, hiring, and overall economic direction. Their importance does not come only from the information itself, but also from the fact that other players watch them closely. Since market participants and institutions incorporate these indicators into their decision-making, they become self-reinforcing and can shape outcomes through collective behavior. 
2. Data Processing
The file all_indicators_monthly_clean.csv serves as the backbone for the regime framework. It combines both leading and coincident indicators into a single monthly panel so that everything can be evaluated on a uniform frequency. Since many macro variables, such as GDP growth or corporate profits, are reported quarterly, I forward-filled those values to a monthly frequency to avoid losing temporal alignment. This step ensures that the dataset captures as much information as possible without breaking the time-series continuity.
Lagging indicators were excluded because they primarily confirm cycles after they have already unfolded. While they are useful for validation, they offer little forward-looking value, and my focus here was on signals that either describe the current state (coincident) or help anticipate what may come next (leading). The result, all_indicators_monthly_clean.csv, is the clean, merged foundation for subsequent analysis.
 
3. Asset Return Panel
The notebook ReturnofAssets.ipynb builds the return dataset that links the macroeconomic regimes to financial market performance. It constructs a monthly return panel for a core set of asset buckets chosen for their relevance to growth, inflation, and investor psychology.
•	Equities (S&P 500): Represent the primary growth-sensitive risk asset. Their performance captures shifts in earnings, sentiment, and overall economic momentum.
•	Treasuries (10Y, duration-proxied): Act as a defensive asset and growth hedge, benefiting when interest rates fall during slowdowns or risk-off episodes.
•	Cash (3M T-bills): Serves as the risk-free anchor and benchmark, providing context for opportunity cost in different regimes.
•	Gold: Functions as a store of value and inflation hedge, especially relevant in high uncertainty, inflationary, or risk-off environments.
I included CPI (headline inflation) and PPI (all commodities) to capture how inflationary pressures interact with these asset classes. CPI reflects the direct impact on consumers and monetary policy, while PPI captures the commodity-driven side of inflation that often bleeds into corporate margins and asset pricing.
Together, this mix covers a broad spectrum of general asset classes. My goal was to create a balanced panel that includes both cyclical growth assets and defensive hedges so I could explore how they react to each other across regimes. By starting with these core building blocks, I can capture the fundamental trade-offs between growth, safety, and inflation that define most macro-driven market cycles.
The outputs of this step include:
•	df_monthly_returns.csv – a clean dataset of monthly returns across all chosen assets.
•	merged_regimes_returns_cleaned.csv – a merged dataset that attaches regime labels to asset returns, making it possible to study how markets behave under different macro environments.
•	regime_analysis_results.csv – summary results showing average returns, as well as frequency of best/worst performance by regime.
4. Indicator Normalization
The notebook Zscore_changer.ipynb standardizes all economic indicators using Z-scores with mean set to 0 and standard deviation set to 1. This step is necessary because the raw data comes in many different forms such as percentages, index levels, and spreads, which cannot be directly compared. By normalizing with Z-scores, each indicator is placed on the same statistical scale so that a one-standard-deviation move in payrolls carries the same weight as a one-standard-deviation move in industrial production or the yield curve. This prevents differences in measurement units from distorting the analysis and produces a clean dataset (all_indicators_zscore.csv) that is ready for modeling.
5. Composite Scoring
Once normalized, each indicator is converted into a 0–100 scaled score, making it easier to interpret and combine into aggregates. Weighting is applied to highlight the relative importance of certain series, such as payrolls and the yield curve, which have historically been among the most powerful cycle indicators. These scaled and weighted indicators are then aggregated into two composite measures. The LeadingScore summarizes forward-looking conditions by blending leading indicators, while the CoincidentScore captures the economy as it currently stands through coincident indicators. This step condenses dozens of moving parts into signal-rich indices that are easier to monitor and analyze. The output (individual_scaled_scores.csv) contains both the individual scaled series and the composite scores. At present, the script requires a fix to properly implement the scaling function, which should rank each series, rescale it to 0–100, and return the transformed version for aggregation.
6. Regime Classification
The notebook EconomicIndicators (1).ipynb expands the framework into a regime-classification system using machine learning. The process begins with feature engineering, where rolling means smooth out noise in the data, and a LeadMinusCoin spread is created to measure the gap between forward-looking and current conditions. Two additional features, LeadMomentum and CoinMomentum, are introduced to capture accelerations or decelerations within the indicators themselves. LeadMomentum measures the short-term changes in leading indicators, highlighting whether predictive signals are strengthening or weakening. CoinMomentum applies the same logic to coincident indicators, tracking whether real-time economic activity is gaining or losing momentum. Together, these features enrich the dataset by embedding information not just about the level of indicators, but also about their directional shifts.
A Gaussian Mixture Model (GMM) is then trained on these features, with tests run across four to eight potential regimes. To prevent excessive switching between classifications, the model’s soft probabilities are smoothed with an Exponentially Weighted Moving Average. The resulting regimes are mapped to intuitive, cycle-consistent labels: Expansion, Early Recovery, Late Cycle, Crisis, Shock Recovery, and Stagnation. The final output (regime_models_labeled.csv) contains smoothed scores, regime labels, and probabilities, providing both a classification system and a measure of confidence in regime identification. This framework ties economic signals to probabilistic regime detection, creating a structured lens through which to interpret market and macro behavior.

<img width="468" height="629" alt="image" src="https://github.com/user-attachments/assets/bc6f69b6-835c-4720-a64a-913173d8abde" />
