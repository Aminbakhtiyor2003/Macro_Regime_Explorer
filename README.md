# regime-eco-data

I would like to start off this repository by noting that this is the first model I have created. My goal here is to bridge together many complex concepts purely through what I have read and studied over the years. I did use AI to assist in the coding process. While I understand the basics of Python, AI helped me implement the structure I had in mind, and I believe the code captures what I intended it to do. Of course, there may still be mistakes in both the coding and conceptual framework, and I am aware of the gaps, for example the manual weighting approach used in grading indicators.


The purpose of this model is to capture changes in leading and coincident economic data sourced from FRED. Although this data is very clean and widely available, meaning the “alpha” is already priced in, it remains extremely important. Businesses, policymakers, and investors rely on this information to guide their decisions, which in turn influences the broader economy and asset prices. My framework takes these economic indicators and compares them against asset performance to see how markets react under different macroeconomic conditions.
This project is designed to create a systematic way of connecting macroeconomic data with financial markets. It collects and processes leading and coincident indicators from FRED, normalizes and aggregates them into composite scores to represent the current and forward-looking state of the economy, and uses statistical and machine learning techniques to classify regimes such as Expansion, Early Recovery, Late Cycle, and Crisis. It then tests how major asset classes, equities, treasuries, cash, and gold, behave in these different regimes. In short, the project is meant to serve as an experimental framework for exploring how the economy and markets interact, and to see what assets perform best under different macroeconomic themes. While it is not a finished or perfect product, it reflects my attempt to put theory into practice and build a foundation for more advanced models in the future.


Now, to be transparent, there are some clear problems with this model. The first is that I rely entirely on clean and public data. Everyone has access to FRED, and most large players have alternative datasets that allow them to estimate or even front-run the official releases of major indicators. As a result, asset prices are often adjusted well before the data even comes out. What actually moves markets is not the data itself, but the surprise, the difference between expectation and reality. In practice, this means that while everyone can brace for the expected, the real volatility comes from when things turn out far better or worse than anticipated. This makes my model more descriptive than predictive, and highlights the psychological dimension of markets.


The second problem is the manual input of weights and indicator selection. This makes the framework highly fragile, as a small change in inputs could completely flip the output, meaning I could just as easily convince myself of patterns that are not really there. It serves as a reminder that with enough tweaking, you can make almost anything look meaningful, something my mom likes to point out when she tells me I read way too deeply into things. In seriousness, I recognize this as an area where machine learning could help automate the weighting process and reduce bias. At this stage, I am still learning, but I plan to explore that path further.
